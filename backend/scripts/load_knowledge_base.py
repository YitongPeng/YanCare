"""
çŸ¥è¯†åº“æ–‡æ¡£åŠ è½½è„šæœ¬
å°† markdown æ–‡æ¡£åˆ‡å—ã€ç”Ÿæˆ embeddingã€å­˜å…¥ ChromaDB
"""
import os
import sys
import re
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from app.services.rag_hybrid import hybrid_rag_service


# çŸ¥è¯†åº“æ–‡æ¡£ç›®å½•
KNOWLEDGE_BASE_DIR = os.path.join(
    os.path.dirname(project_root),
    "çŸ¥è¯†åº“æ–‡æ¡£"
)


class DocumentLoader:
    """æ–‡æ¡£åŠ è½½å™¨"""
    
    def __init__(self, kb_dir: str):
        self.kb_dir = kb_dir
        self.documents = []
    
    def load_all_documents(self):
        """åŠ è½½æ‰€æœ‰markdownæ–‡æ¡£"""
        print(f"ğŸ“‚ æ­£åœ¨ä»ç›®å½•åŠ è½½æ–‡æ¡£: {self.kb_dir}")
        
        if not os.path.exists(self.kb_dir):
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {self.kb_dir}")
            return []
        
        # æ–‡æ¡£æ–‡ä»¶æ˜ å°„
        doc_files = {
            "1-å¸¸è§é—®é¢˜FAQ.md": "FAQ",
            "2-å¡ç§ä»‹ç».md": "å¡ç§",
            "3-æœåŠ¡é¡¹ç›®ä»‹ç».md": "æœåŠ¡",
            "4-äº§å“ä»‹ç».md": "äº§å“",
            "5-é—¨åº—ä¿¡æ¯.md": "é—¨åº—",
            "6-å…»å‘çŸ¥è¯†ç§‘æ™®.md": "å…»å‘çŸ¥è¯†",
            "7-é¢„çº¦å’Œä½¿ç”¨æŒ‡å—.md": "é¢„çº¦",
        }
        
        for filename, category in doc_files.items():
            filepath = os.path.join(self.kb_dir, filename)
            if os.path.exists(filepath):
                print(f"  ğŸ“„ åŠ è½½: {filename}")
                self._load_single_document(filepath, category)
            else:
                print(f"  âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
        
        print(f"âœ… å…±åŠ è½½ {len(self.documents)} ä¸ªæ–‡æ¡£å—\n")
        return self.documents
    
    def _load_single_document(self, filepath: str, category: str):
        """åŠ è½½å•ä¸ªæ–‡æ¡£å¹¶åˆ‡å—"""
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # è·å–æ–‡æ¡£åï¼ˆä¸å«æ‰©å±•åï¼‰
        doc_name = os.path.splitext(os.path.basename(filepath))[0]
        
        # æ ¹æ®æ–‡æ¡£ç±»å‹é€‰æ‹©åˆ‡å—ç­–ç•¥
        if category == "FAQ":
            chunks = self._chunk_faq(content, doc_name, category)
        else:
            chunks = self._chunk_by_section(content, doc_name, category)
        
        self.documents.extend(chunks)
    
    def _chunk_faq(self, content: str, doc_name: str, category: str) -> list:
        """
        FAQ æ–‡æ¡£æŒ‰é—®ç­”å¯¹åˆ‡å—
        æ¯ä¸ª Q&A æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ chunk
        """
        chunks = []
        
        # æŒ‰ ### Q åˆ†å‰²ï¼ˆä¿®å¤ï¼šåªéœ€è¦ä¸€ä¸ªæ¢è¡Œç¬¦ï¼‰
        qa_pattern = r'###\s*Q\d+[ï¼š:]\s*(.+?)\n\*\*ç­”[ï¼š:]\*\*\s*(.+?)(?=\n###|$)'
        matches = re.findall(qa_pattern, content, re.DOTALL)
        
        for i, (question, answer) in enumerate(matches):
            question = question.strip()
            answer = answer.strip()
            
            # æ¸…ç† Markdown æ ¼å¼ç¬¦å·
            question = self._clean_markdown(question)
            answer = self._clean_markdown(answer)
            
            # ç»„åˆé—®ç­”
            chunk_content = f"é—®ï¼š{question}\nç­”ï¼š{answer}"
            
            chunks.append({
                "id": f"{doc_name}_q{i+1}",
                "content": chunk_content,
                "metadata": {
                    "source": doc_name,
                    "category": category,
                    "type": "faq",
                    "question": question,
                    "chunk_index": i
                }
            })
        
        print(f"    âœ‚ï¸  åˆ‡å—: {len(chunks)} ä¸ªé—®ç­”å¯¹")
        return chunks
    
    def _chunk_by_section(self, content: str, doc_name: str, category: str) -> list:
        """
        æ™®é€šæ–‡æ¡£æŒ‰ç« èŠ‚åˆ‡å—
        æ¯ä¸ª ## æˆ– ### æ˜¯ä¸€ä¸ª chunk
        """
        chunks = []
        
        # æŒ‰æ ‡é¢˜åˆ†å‰²ï¼ˆ## æˆ– ###ï¼‰
        sections = re.split(r'\n(?=##\s)', content)
        
        for i, section in enumerate(sections):
            section = section.strip()
            
            # è·³è¿‡ç©ºç« èŠ‚å’Œå¤ªçŸ­çš„å†…å®¹
            if not section or len(section) < 50:
                continue
            
            # æå–æ ‡é¢˜
            title_match = re.match(r'##\s*(.+)', section)
            title = title_match.group(1).strip() if title_match else "æœªå‘½åç« èŠ‚"
            
            # æ¸…ç† Markdown æ ¼å¼
            section_clean = self._clean_markdown(section)
            
            # å¦‚æœç« èŠ‚è¿‡é•¿ï¼ˆ>800å­—ï¼‰ï¼Œè¿›ä¸€æ­¥åˆ‡åˆ†
            if len(section_clean) > 800:
                sub_chunks = self._split_long_section(section_clean, doc_name, category, title, i)
                chunks.extend(sub_chunks)
            else:
                chunks.append({
                    "id": f"{doc_name}_sec{i}",
                    "content": section_clean,
                    "metadata": {
                        "source": doc_name,
                        "category": category,
                        "type": "section",
                        "title": title,
                        "chunk_index": i
                    }
                })
        
        print(f"    âœ‚ï¸  åˆ‡å—: {len(chunks)} ä¸ªç« èŠ‚")
        return chunks
    
    def _clean_markdown(self, text: str) -> str:
        """
        æ¸…ç† Markdown æ ¼å¼ç¬¦å·ï¼Œä¿ç•™çº¯æ–‡æœ¬
        """
        # å»é™¤åŠ ç²—ç¬¦å· **text**
        text = re.sub(r'\*\*(.+?)\*\*', r'\1', text)
        
        # å»é™¤æ–œä½“ç¬¦å· *text* æˆ– _text_
        text = re.sub(r'\*(.+?)\*', r'\1', text)
        text = re.sub(r'_(.+?)_', r'\1', text)
        
        # å»é™¤é“¾æ¥ [text](url)ï¼Œä¿ç•™ text
        text = re.sub(r'\[(.+?)\]\(.+?\)', r'\1', text)
        
        # å»é™¤æ ‡é¢˜ç¬¦å· ## ã€### ç­‰
        text = re.sub(r'^#{1,6}\s+', '', text, flags=re.MULTILINE)
        
        # å»é™¤åˆ—è¡¨ç¬¦å· - æˆ– * æˆ– æ•°å­—.
        text = re.sub(r'^[\-\*]\s+', '', text, flags=re.MULTILINE)
        text = re.sub(r'^\d+\.\s+', '', text, flags=re.MULTILINE)
        
        # å»é™¤ä»£ç å—ç¬¦å· ``` 
        text = re.sub(r'```[\s\S]*?```', '', text)
        text = re.sub(r'`(.+?)`', r'\1', text)
        
        return text
    
    def _split_long_section(
        self, 
        section: str, 
        doc_name: str, 
        category: str, 
        title: str, 
        section_idx: int
    ) -> list:
        """å°†è¿‡é•¿çš„ç« èŠ‚è¿›ä¸€æ­¥åˆ‡åˆ†"""
        chunks = []
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = section.split('\n\n')
        
        current_chunk = ""
        chunk_count = 0
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
            
            # å¦‚æœåŠ ä¸Šå½“å‰æ®µè½ä¸è¶…è¿‡600å­—ï¼Œç»§ç»­ç´¯ç§¯
            if len(current_chunk) + len(para) < 600:
                current_chunk += para + "\n\n"
            else:
                # ä¿å­˜å½“å‰chunk
                if current_chunk:
                    chunks.append({
                        "id": f"{doc_name}_sec{section_idx}_sub{chunk_count}",
                        "content": current_chunk.strip(),
                        "metadata": {
                            "source": doc_name,
                            "category": category,
                            "type": "section",
                            "title": title,
                            "chunk_index": section_idx,
                            "sub_index": chunk_count
                        }
                    })
                    chunk_count += 1
                
                # å¼€å§‹æ–°chunk
                current_chunk = para + "\n\n"
        
        # ä¿å­˜æœ€åä¸€ä¸ªchunk
        if current_chunk:
            chunks.append({
                "id": f"{doc_name}_sec{section_idx}_sub{chunk_count}",
                "content": current_chunk.strip(),
                "metadata": {
                    "source": doc_name,
                    "category": category,
                    "type": "section",
                    "title": title,
                    "chunk_index": section_idx,
                    "sub_index": chunk_count
                }
            })
        
        return chunks


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸš€ ç‡•æ–›å ‚çŸ¥è¯†åº“åŠ è½½å·¥å…·")
    print("=" * 60)
    print()
    
    # 1. åŠ è½½æ–‡æ¡£
    loader = DocumentLoader(KNOWLEDGE_BASE_DIR)
    documents = loader.load_all_documents()
    
    if not documents:
        print("âŒ æ²¡æœ‰æ–‡æ¡£å¯åŠ è½½ï¼Œé€€å‡º")
        return
    
    # 2. æ¸…ç©ºæ—§æ•°æ®ï¼ˆå¯é€‰ï¼‰
    print("ğŸ—‘ï¸  å‡†å¤‡æ¸…ç©ºæ—§çŸ¥è¯†åº“...")
    try:
        collection = hybrid_rag_service.collection
        if collection and collection.count() > 0:
            # è·å–æ‰€æœ‰IDå¹¶åˆ é™¤
            all_docs = collection.get()
            if all_docs and all_docs["ids"]:
                collection.delete(ids=all_docs["ids"])
                print(f"âœ… å·²æ¸…ç©º {len(all_docs['ids'])} ä¸ªæ—§æ–‡æ¡£\n")
    except Exception as e:
        print(f"âš ï¸  æ¸…ç©ºæ—§æ•°æ®å¤±è´¥: {e}\n")
    
    # 3. å†™å…¥æ–°æ•°æ®
    print("ğŸ’¾ æ­£åœ¨å†™å…¥çŸ¥è¯†åº“...")
    try:
        collection = hybrid_rag_service.collection
        if collection is None:
            print("âŒ æ— æ³•è·å– collectionï¼Œè¯·æ£€æŸ¥ embedding æ¨¡å‹")
            return
        
        # æ‰¹é‡å†™å…¥
        ids = [doc["id"] for doc in documents]
        contents = [doc["content"] for doc in documents]
        metadatas = [doc["metadata"] for doc in documents]
        
        collection.add(
            ids=ids,
            documents=contents,
            metadatas=metadatas
        )
        
        print(f"âœ… æˆåŠŸå†™å…¥ {len(documents)} ä¸ªæ–‡æ¡£å—\n")
        
    except Exception as e:
        print(f"âŒ å†™å…¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 4. éªŒè¯
    print("ğŸ” éªŒè¯çŸ¥è¯†åº“...")
    doc_count = hybrid_rag_service.get_count()
    print(f"âœ… å½“å‰çŸ¥è¯†åº“æ–‡æ¡£æ•°: {doc_count}\n")
    
    # 5. æµ‹è¯•æ£€ç´¢
    print("ğŸ§ª æµ‹è¯•æ£€ç´¢åŠŸèƒ½...")
    test_queries = [
        "ä½ ä»¬æœ‰ä»€ä¹ˆå¡ï¼Ÿ",
        "é—¨åº—åœ°å€åœ¨å“ªï¼Ÿ",
        "æ€ä¹ˆé¢„çº¦ï¼Ÿ"
    ]
    
    for query in test_queries:
        print(f"\næŸ¥è¯¢: {query}")
        results = hybrid_rag_service.search(query, n_results=2)
        for i, doc in enumerate(results, 1):
            print(f"  {i}. [{doc['metadata'].get('category', 'unknown')}] {doc['content'][:50]}...")
    
    print("\n" + "=" * 60)
    print("ğŸ‰ çŸ¥è¯†åº“åŠ è½½å®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    main()
